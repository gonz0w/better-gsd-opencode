---
phase: 48-compare-multi-attempt-metrics-aggregation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/commands/trajectory.js
  - src/router.js
  - src/lib/constants.js
autonomous: true
requirements:
  - COMP-01
  - COMP-02
  - COMP-03
  - COMP-04
  - COMP-05

must_haves:
  truths:
    - "User can run `trajectory compare <name>` and see test results, LOC delta, and complexity for each attempt"
    - "Compare identifies the best attempt per metric (green) and worst attempt per metric (red)"
    - "Piped output produces structured JSON with attempts array and best/worst indices"
    - "Missing or null metrics display as '-' without crashing"
    - "Abandoned entries are excluded from comparison"
  artifacts:
    - path: "src/commands/trajectory.js"
      provides: "cmdTrajectoryCompare and formatCompareResult functions"
      exports: ["cmdTrajectoryCompare"]
    - path: "src/router.js"
      provides: "Route trajectory compare to handler"
      contains: "case 'compare'"
    - path: "src/lib/constants.js"
      provides: "Help text for trajectory compare"
      contains: "trajectory compare"
  key_links:
    - from: "src/router.js"
      to: "src/commands/trajectory.js"
      via: "lazyTrajectory().cmdTrajectoryCompare"
      pattern: "cmdTrajectoryCompare"
    - from: "src/commands/trajectory.js"
      to: "src/lib/output.js"
      via: "output(result, { formatter })"
      pattern: "output\\(result.*formatter.*formatCompareResult"
    - from: "src/commands/trajectory.js"
      to: "src/lib/format.js"
      via: "formatTable with colorFn for green/red coloring"
      pattern: "formatTable.*colorFn"
---

<objective>
Implement the `trajectory compare` subcommand that reads checkpoint journal entries, aggregates metrics across attempts, identifies best/worst per metric, and renders a color-coded comparison matrix.

Purpose: Enables data-driven comparison of exploration attempts so developers can make informed decisions about which approach to pursue (DO-23).
Output: Working `trajectory compare <name>` command with TTY color table and JSON fallback.
</objective>

<execution_context>
@/home/cam/.config/oc/get-shit-done/workflows/execute-plan.md
@/home/cam/.config/oc/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/48-compare-multi-attempt-metrics-aggregation/48-RESEARCH.md
@src/commands/trajectory.js
@src/router.js
@src/lib/constants.js
@src/lib/format.js
@src/lib/output.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement cmdTrajectoryCompare and formatCompareResult</name>
  <files>src/commands/trajectory.js</files>
  <action>
Add two new functions to `src/commands/trajectory.js` after the pivot command section:

**`cmdTrajectoryCompare(cwd, args, raw)`** — Read-only command that:
1. Parses args: positional name (required), `--scope` flag (default: 'phase')
2. Reads `.planning/memory/trajectory.json` (same pattern as cmdTrajectoryList)
3. Filters to entries matching: `category === 'checkpoint'`, `checkpoint_name === name`, `scope === scope`, AND excludes entries tagged 'abandoned' (`!(e.tags && e.tags.includes('abandoned'))`)
4. Sorts by `attempt` ascending
5. If no matching entries found, call `error()` with helpful message listing available checkpoint names (same pattern as pivot's not-found handling)
6. Builds metrics array from entries, using optional chaining with nullish coalescing for each metric field:
   - `tests_pass`: `e.metrics?.tests?.pass ?? null`
   - `tests_fail`: `e.metrics?.tests?.fail ?? null`
   - `tests_total`: `e.metrics?.tests?.total ?? null`
   - `loc_insertions`: `e.metrics?.loc_delta?.insertions ?? null`
   - `loc_deletions`: `e.metrics?.loc_delta?.deletions ?? null`
   - `complexity`: `e.metrics?.complexity?.total ?? null`
   - Also include: `attempt`, `branch`, `git_ref`, `timestamp`
7. Computes best/worst index per metric using direction rules:
   - `tests_pass`: higher is better
   - `tests_fail`: lower is better
   - `loc_insertions`: lower is better
   - `loc_deletions`: lower is better
   - `complexity`: lower is better
   - Only assign best/worst if bestIdx !== worstIdx (skip coloring for single-attempt or tied values)
   - Skip null values when computing best/worst
8. Produces result object: `{ checkpoint, scope, attempt_count, attempts, best_per_metric, worst_per_metric }`
9. Calls `output(result, { formatter: formatCompareResult })`

**`formatCompareResult(result)`** — TTY formatter that:
1. Renders `banner('TRAJECTORY COMPARE')`
2. Shows checkpoint name (bold) and scope, attempt count
3. Builds table with headers: `['Attempt', 'Tests Pass', 'Tests Fail', 'LOC +/-', 'Complexity']`
4. Rows: attempt label `#N`, test values as strings or '-' for null, LOC as `+insertions/-deletions` composite string, complexity as string or '-'
5. Uses `formatTable` with `colorFn` option: for each cell, check if row index matches `best_per_metric[metric]` → `color.green()`, or `worst_per_metric[metric]` → `color.red()`. Column 0 (attempt label) is uncolored. LOC column (index 3) uses `loc_insertions` for best/worst determination.
6. Adds `summaryLine('Best attempt per metric highlighted in green')` 
7. Adds `actionHint('trajectory choose <attempt-N>')` at the bottom

**Update `module.exports`** to include `cmdTrajectoryCompare`.

IMPORTANT: Do NOT use `colorFn` to return both the colored AND formatted string — the `colorFn` return value IS the cell text per format.js implementation. Let colorFn handle the raw value, not the pre-formatted string (see Research pitfall #3).

For the LOC column, format the cell value as `+${insertions}/-${deletions}` in the rows array, and in colorFn for column 3, use the best/worst of `loc_insertions` to determine color.
  </action>
  <verify>
Run `npm run build` — should succeed without errors.
Check bundle size stays under 1050KB budget.
Run `node bin/gsd-tools.cjs trajectory compare --help` — should print help text (will add in Task 2).
  </verify>
  <done>
cmdTrajectoryCompare and formatCompareResult exist in trajectory.js. Module exports include cmdTrajectoryCompare.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire router and add help text</name>
  <files>src/router.js, src/lib/constants.js</files>
  <action>
**In `src/router.js`:**
Add `compare` case to the trajectory switch block (line ~927-934), following the existing pattern:
```
case 'compare': lazyTrajectory().cmdTrajectoryCompare(cwd, args.slice(1), raw); break;
```
Note: pass `args.slice(1)` (not `args.slice(2)`) to be consistent with how checkpoint receives args — but actually check: checkpoint passes `args.slice(1)` where args[0] is already the subcommand name. For compare, `args` in the trajectory switch starts from the full argv, so `args[0]='trajectory'`, `args[1]='compare'`. The slice passed should be `args.slice(1)` so that the compare handler receives `['compare', '<name>', ...]`. This matches how checkpoint and pivot receive their args.

Update the default error message to include 'compare' in the available subcommands list:
`error('Unknown trajectory subcommand: ' + trajSub + '. Available: checkpoint, list, pivot, compare')`

**In `src/lib/constants.js`:**
1. Update the existing `'trajectory'` help entry to add the compare subcommand:
   - Add to the Subcommands section: `compare <name>` with options `--scope <scope>`
   - Add an example: `gsd-tools trajectory compare my-feat`
   
2. Add a new compound help key `'trajectory compare'` with detailed usage:
```
Usage: gsd-tools trajectory compare <name> [--scope <scope>]

Compare metrics across all attempts for a named checkpoint.
Shows test results, LOC delta, and cyclomatic complexity side-by-side.
Best values highlighted green, worst highlighted red.

Arguments:
  name              Checkpoint name to compare attempts for

Options:
  --scope <scope>   Scope level (default: phase)

Output: { checkpoint, scope, attempt_count, attempts, best_per_metric, worst_per_metric }

Examples:
  gsd-tools trajectory compare my-feat
  gsd-tools trajectory compare try-redis --scope task
```
  </action>
  <verify>
Run `npm run build` — should succeed.
Run `node bin/gsd-tools.cjs trajectory compare --help` — should print the compare help text to stderr.
Run `node bin/gsd-tools.cjs trajectory --help` — should list compare in available subcommands.
  </verify>
  <done>
Router dispatches `trajectory compare` to handler. Help text exists for both `trajectory` (updated) and `trajectory compare` (new compound key). Build passes.
  </done>
</task>

</tasks>

<verification>
- `npm run build` succeeds with bundle under 1050KB
- `node bin/gsd-tools.cjs trajectory compare --help` prints usage info
- `node bin/gsd-tools.cjs trajectory --help` lists compare subcommand
- Manual test: create a trajectory.json with test entries, run `trajectory compare <name>` in JSON mode, verify output schema matches expected format
</verification>

<success_criteria>
- cmdTrajectoryCompare reads journal entries, filters non-abandoned checkpoints, aggregates metrics, identifies best/worst per metric
- formatCompareResult renders color-coded table (green=best, red=worst) with formatTable colorFn
- Router dispatches `trajectory compare` correctly
- Help text present for both `trajectory` and `trajectory compare`
- Build succeeds under 1050KB budget
</success_criteria>

<output>
After completion, create `.planning/phases/48-compare-multi-attempt-metrics-aggregation/48-01-SUMMARY.md`
</output>
