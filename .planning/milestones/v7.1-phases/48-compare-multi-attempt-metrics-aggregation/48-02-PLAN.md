---
phase: 48-compare-multi-attempt-metrics-aggregation
plan: 02
type: execute
wave: 2
depends_on:
  - 48-01
files_modified:
  - bin/gsd-tools.test.cjs
autonomous: true
requirements:
  - COMP-01
  - COMP-02
  - COMP-03
  - COMP-04
  - COMP-05

must_haves:
  truths:
    - "Tests prove compare shows test results (pass/fail) across attempts"
    - "Tests prove compare shows LOC delta across attempts"
    - "Tests prove compare shows complexity across attempts"
    - "Tests prove compare identifies best/worst per metric correctly"
    - "Tests prove piped output produces valid JSON with expected schema"
    - "Tests prove null/missing metrics handled gracefully"
    - "Tests prove abandoned entries are excluded"
  artifacts:
    - path: "bin/gsd-tools.test.cjs"
      provides: "Comprehensive trajectory compare test suite"
      contains: "trajectory compare"
  key_links:
    - from: "bin/gsd-tools.test.cjs"
      to: "src/commands/trajectory.js"
      via: "runGsdTools('trajectory compare ...')"
      pattern: "trajectory compare"
---

<objective>
Add comprehensive tests for the `trajectory compare` command covering all five COMP requirements: test metrics, LOC delta, complexity, best/worst aggregation matrix, and JSON output format.

Purpose: Validates that the compare command correctly aggregates and presents multi-attempt metrics for informed exploration decisions.
Output: Test suite passing with trajectory compare coverage.
</objective>

<execution_context>
@/home/cam/.config/oc/get-shit-done/workflows/execute-plan.md
@/home/cam/.config/oc/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/48-compare-multi-attempt-metrics-aggregation/48-RESEARCH.md
@.planning/phases/48-compare-multi-attempt-metrics-aggregation/48-01-SUMMARY.md
@bin/gsd-tools.test.cjs
@src/commands/trajectory.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add trajectory compare test suite</name>
  <files>bin/gsd-tools.test.cjs</files>
  <action>
Add a `describe('trajectory compare', ...)` test block after the existing `trajectory pivot` tests (around line 16810+). Follow the established test patterns from trajectory checkpoint/pivot tests.

**Helper functions** (add at top of describe block):
- `initGitForCompare(dir)` — creates `.planning/memory/` dir, dummy.txt, git init + config + initial commit (identical pattern to `initGitForCheckpoint`)
- `writeTrajectoryEntries(dir, entries)` — writes entries array to `.planning/memory/trajectory.json`

**Test cases (each validates specific requirements):**

1. **COMP-01: Compare shows test results across attempts**
   - Write 2 checkpoint entries with different test metrics (e.g., attempt 1: 95/100 pass, attempt 2: 100/100 pass)
   - Run `trajectory compare <name>` (piped = JSON output)
   - Assert: output has `attempt_count: 2`, attempts[0].tests_pass === 95, attempts[1].tests_pass === 100

2. **COMP-02: Compare shows LOC delta across attempts**
   - Write 2 entries with different loc_delta metrics (e.g., attempt 1: +50/-10, attempt 2: +30/-5)
   - Assert: attempts[0].loc_insertions === 50, attempts[1].loc_insertions === 30

3. **COMP-03: Compare shows complexity across attempts**
   - Write 2 entries with different complexity (e.g., attempt 1: total 15, attempt 2: total 12)
   - Assert: attempts[0].complexity === 15, attempts[1].complexity === 12

4. **COMP-04: Best/worst identification per metric**
   - Write 2 entries: attempt 1 has more tests passing but higher complexity, attempt 2 has fewer tests but lower complexity
   - Assert: `best_per_metric.tests_pass === 0` (attempt 1 index), `best_per_metric.complexity === 1` (attempt 2 index)
   - Assert: `worst_per_metric.tests_pass === 1`, `worst_per_metric.complexity === 0`

5. **COMP-05: JSON output schema validation**
   - Write 2 entries, run compare
   - Assert: output has fields: `checkpoint`, `scope`, `attempt_count`, `attempts` (array), `best_per_metric` (object), `worst_per_metric` (object)
   - Each attempt has: `attempt`, `branch`, `git_ref`, `timestamp`, `tests_pass`, `tests_fail`, `tests_total`, `loc_insertions`, `loc_deletions`, `complexity`

6. **Null metrics handled gracefully**
   - Write 2 entries: attempt 1 has full metrics, attempt 2 has `metrics: null`
   - Assert: attempt 2 values are all null, command doesn't crash

7. **Abandoned entries excluded from comparison**
   - Write 3 entries: attempts 1 and 3 normal, attempt 2 tagged `['checkpoint', 'abandoned']`
   - Assert: `attempt_count === 2` (abandoned excluded)

8. **Single attempt shows data without best/worst**
   - Write 1 entry only
   - Assert: `attempt_count === 1`, `best_per_metric` is empty object (no comparison to make)

9. **Missing checkpoint name errors**
   - Run `trajectory compare` with no name
   - Assert: command fails (result.success === false)

10. **Non-existent checkpoint name errors**
    - Run `trajectory compare nonexistent`
    - Assert: command fails with error message

11. **Scope filtering works**
    - Write entries for same name but different scopes (phase vs task)
    - Run `trajectory compare <name> --scope task`
    - Assert: only task-scoped entries returned

**Journal entry template** (reuse across tests):
```javascript
{
  id: 'tj-001', category: 'checkpoint', checkpoint_name: 'my-feat',
  scope: 'phase', attempt: 1,
  branch: 'trajectory/phase/my-feat/attempt-1',
  git_ref: 'abc1234',
  timestamp: '2026-03-01T01:00:00Z',
  metrics: {
    tests: { total: 100, pass: 95, fail: 5 },
    loc_delta: { insertions: 50, deletions: 10, files_changed: 4 },
    complexity: { total: 15, files_analyzed: 3 }
  },
  tags: ['checkpoint']
}
```
  </action>
  <verify>
Run `npm test` — all tests pass including new trajectory compare tests.
Run `npm test 2>&1 | grep "trajectory compare"` — confirm compare tests are executing.
Check total test count increased (was 728 after phase 47).
  </verify>
  <done>
All trajectory compare tests pass. Tests cover: test metrics (COMP-01), LOC delta (COMP-02), complexity (COMP-03), best/worst matrix (COMP-04), JSON output schema (COMP-05), null handling, abandoned exclusion, single attempt, error cases, scope filtering.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run full test suite and verify build</name>
  <files>bin/gsd-tools.test.cjs</files>
  <action>
Run the complete test suite to verify zero regressions:
1. `npm test` — all tests pass (expect 738+ total: 728 existing + 10+ new compare tests)
2. `npm run build` — build succeeds, bundle under 1050KB
3. Verify the trajectory compare help text works: `node bin/gsd-tools.cjs trajectory compare --help`

If any test failures occur:
- Fix the failing tests or implementation code
- Re-run until green
- Do NOT skip or disable existing tests

If bundle size exceeds 1050KB:
- Inline format helpers rather than adding new functions
- Combine LOC insertions/deletions into a single display string to reduce branching
  </action>
  <verify>
`npm test` exits 0 with all tests passing.
`npm run build` exits 0 with bundle size under 1050KB.
  </verify>
  <done>
Full test suite passes with zero regressions. Build succeeds within bundle budget. Trajectory compare command is fully functional.
  </done>
</task>

</tasks>

<verification>
- `npm test` passes with 738+ tests (zero regressions from 728 baseline)
- All 11 trajectory compare test cases pass
- `npm run build` succeeds under 1050KB budget
- `node bin/gsd-tools.cjs trajectory compare --help` outputs usage info
</verification>

<success_criteria>
- 10+ new tests covering all COMP-01 through COMP-05 requirements
- Zero test regressions from Phase 47 baseline (728 tests)
- Tests prove: metrics aggregation, best/worst identification, null handling, abandoned exclusion, scope filtering, error cases
- Full build passes
</success_criteria>

<output>
After completion, create `.planning/phases/48-compare-multi-attempt-metrics-aggregation/48-02-SUMMARY.md`
</output>
